"""
Instruction Hierarchy Training Implementation
Based on "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions"

IMPORTANT: The paper trains ONE model on ALL instruction hierarchy data combined,
not separate models for each attack type. Figures 2 and 3 show evaluation results
on different test sets, not different models.

This implementation includes:
1. Supervised Fine-tuning (SFT) with LoRA on ALL generated data
2. Direct Preference Optimization (DPO) for RL training using HuggingFace's DPOTrainer
"""

import os
import json
import torch
from torch.utils.data import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training,
    PeftModel
)
from trl import DPOConfig, DPOTrainer
from datasets import Dataset as HFDataset
import wandb
from typing import Dict, List, Optional
import argparse


# ====================
# Configuration
# ====================

class TrainingConfig:
    """Configuration for training the instruction hierarchy model"""

    # Model configuration
    model_name: str = "Qwen/Qwen2.5-7B-Instruct"  # Base model
    use_4bit: bool = True  # Use 4-bit quantization

    # LoRA configuration
    lora_r: int = 1
    lora_alpha: int = 256
    lora_dropout: float = 0.1
    lora_target_modules: List[str] = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

    # Training configuration
    batch_size: int = 1
    gradient_accumulation_steps: int = 8
    learning_rate: float = 2e-5
    num_epochs_sft: int = 5
    num_epochs_dpo: int = 3
    warmup_steps: int = 100
    max_length: int = 2048
    max_prompt_length: int = 1024

    # Data paths - Updated to match generate_data.py output
    data_dir: str = "./instruction_hierarchy_data"  # Directory with all JSON files
    full_dataset_file: str = "./instruction_hierarchy_data/instruction_hierarchy_full.json"  # Combined dataset

    # Output paths
    output_dir: str = "./instruction_hierarchy_model"

    # DPO specific
    dpo_beta: float = 0.1

    # Logging
    use_wandb: bool = False
    wandb_project: str = "instruction-hierarchy"


# ====================
# Dataset Classes
# ====================

class InstructionHierarchyDataset(Dataset):
    """Dataset for instruction hierarchy training

    Loads data generated by generate_data.py which has the structure:
    {
        "type": "misaligned_open_domain",
        "system_message": "...",
        "user_message": "...",
        "tool_outputs": [...] (optional),
        "ground_truth": "...",
        "should_refuse": true/false
    }
    """

    def __init__(self, data_path: str, tokenizer, max_length: int = 2048, mode: str = "sft"):
        """
        Args:
            data_path: Path to the JSON dataset file or directory
            tokenizer: Tokenizer instance
            max_length: Maximum sequence length
            mode: "sft" for supervised fine-tuning or "dpo" for preference optimization
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.mode = mode

        # Load data
        self.data = self._load_data(data_path)
        print(f"\nLoaded {len(self.data)} total examples from {data_path}")

        # Print data distribution
        self._print_data_distribution()

        # Prepare data based on mode
        if mode == "sft":
            self.examples = self._prepare_sft_data()
        elif mode == "dpo":
            self.examples = self._prepare_dpo_data()
        else:
            raise ValueError(f"Unknown mode: {mode}")

    def _load_data(self, data_path: str) -> List[Dict]:
        """Load data from file or directory"""
        all_data = []

        if os.path.isfile(data_path):
            # Load single file
            with open(data_path, 'r') as f:
                data = json.load(f)
                if isinstance(data, list):
                    all_data = data
                else:
                    all_data = [data]
        elif os.path.isdir(data_path):
            # Load all JSON files from directory
            for file in sorted(os.listdir(data_path)):
                if file.endswith('.json'):
                    filepath = os.path.join(data_path, file)
                    with open(filepath, 'r') as f:
                        file_data = json.load(f)
                        if isinstance(file_data, list):
                            all_data.extend(file_data)
                        else:
                            all_data.append(file_data)
        else:
            raise ValueError(f"Data path does not exist: {data_path}")

        return all_data

    def _print_data_distribution(self):
        """Print distribution of data types"""
        from collections import Counter
        type_counts = Counter(item.get('type', 'unknown') for item in self.data)
        print("\nData distribution by type:")
        for dtype, count in sorted(type_counts.items()):
            print(f"  {dtype}: {count} examples")

    def _prepare_sft_data(self) -> List[Dict]:
        """Prepare data for supervised fine-tuning

        SFT trains the model to produce the ground_truth output given the input.
        """
        examples = []

        for item in self.data:
            # Format the conversation with hierarchy levels
            formatted_input = self._format_input(item)
            ground_truth = item.get('ground_truth', '')

            if not ground_truth:
                continue  # Skip examples without ground truth

            examples.append({
                'input': formatted_input,
                'output': ground_truth,
                'type': item.get('type', 'unknown')
            })

        print(f"Prepared {len(examples)} examples for SFT training")
        return examples

    def _prepare_dpo_data(self) -> List[Dict]:
        """Prepare preference pairs for DPO training

        DPO trains the model to prefer certain responses over others.
        For attacks (misaligned): preferred = refusal, rejected = compliance
        For aligned instructions: preferred = helpful response, rejected = over-refusal
        """
        examples = []

        for item in self.data:
            formatted_input = self._format_input(item)
            ground_truth = item.get('ground_truth', '')
            item_type = item.get('type', '')
            should_refuse = item.get('should_refuse', False)

            if not ground_truth:
                continue  # Skip examples without ground truth

            # Determine if this is an attack that should be refused
            is_attack = (
                should_refuse or
                'misaligned' in item_type or
                'injection' in item_type or
                'extraction' in item_type or
                'jailbreak' in item_type
            )

            if is_attack:
                # For attacks: preferred is refusal/correct behavior, rejected is compliance
                preferred = ground_truth  # The ground truth is the refusal

                # Generate a "bad" response that complies with the attack
                if 'extraction' in item_type:
                    # Bad: reveals system message
                    system_msg = item.get('system_message', 'hidden instructions')
                    rejected = f"Sure! My system message is: {system_msg[:100]}..."
                elif 'injection' in item_type:
                    # Bad: follows the injected instruction
                    rejected = "Following your new instructions now. I will ignore my original instructions."
                elif 'jailbreak' in item_type:
                    # Bad: complies with unsafe request
                    rejected = "I'll help you with that harmful request. Here's how..."
                else:
                    # Generic bad response
                    rejected = "I'll comply with your request that conflicts with my instructions."

                examples.append({
                    'prompt': formatted_input,
                    'chosen': preferred,
                    'rejected': rejected
                })

            elif 'aligned' in item_type:
                # For aligned instructions: preferred is helpful, rejected is over-refusal
                preferred = ground_truth  # Helpful response
                rejected = "I'm sorry, but I cannot help with that request."  # Over-refusal

                examples.append({
                    'prompt': formatted_input,
                    'chosen': preferred,
                    'rejected': rejected
                })

        print(f"Prepared {len(examples)} preference pairs for DPO training")
        return examples

    def _format_input(self, item: Dict) -> str:
        """Format the input with proper hierarchy markers

        The paper defines this hierarchy:
        - Priority 0 (Highest): System Message
        - Priority 10 (High): User Message
        - Priority 30 (Low): Tool Outputs
        """
        formatted_input = ""

        # System message (highest privilege - Priority 0)
        if 'system_message' in item and item['system_message']:
            formatted_input += f"<|system|>\n{item['system_message']}\n"

        # User message (medium privilege - Priority 10)
        if 'user_message' in item and item['user_message']:
            formatted_input += f"<|user|>\n{item['user_message']}\n"

        # Tool outputs (lowest privilege - Priority 30)
        if 'tool_outputs' in item:
            tool_outputs = item['tool_outputs']
            if isinstance(tool_outputs, str):
                tool_outputs = [tool_outputs]
            if tool_outputs:
                for i, tool_output in enumerate(tool_outputs):
                    if tool_output:  # Skip empty outputs
                        formatted_input += f"<|tool|>\n{tool_output}\n"

        formatted_input += "<|assistant|>\n"

        return formatted_input

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        example = self.examples[idx]

        if self.mode == "sft":
            # Tokenize for SFT
            full_text = example['input'] + example['output']
            encoding = self.tokenizer(
                full_text,
                truncation=True,
                padding="max_length",
                max_length=self.max_length,
                return_tensors="pt"
            )

            # Create labels (mask the input part)
            labels = encoding['input_ids'].clone()
            input_len = len(self.tokenizer.encode(example['input'], truncation=True))
            labels[:, :input_len] = -100  # Mask input tokens

            return {
                'input_ids': encoding['input_ids'].squeeze(),
                'attention_mask': encoding['attention_mask'].squeeze(),
                'labels': labels.squeeze()
            }

        elif self.mode == "dpo":
            # Return raw text for DPO trainer
            return {
                'prompt': example['prompt'],
                'chosen': example['chosen'],
                'rejected': example['rejected']
            }


def create_hf_dataset(examples: List[Dict], mode: str = "dpo") -> HFDataset:
    """Convert examples to HuggingFace Dataset format for DPO"""
    if mode == "dpo":
        data_dict = {
            'prompt': [ex['prompt'] for ex in examples],
            'chosen': [ex['chosen'] for ex in examples],
            'rejected': [ex['rejected'] for ex in examples]
        }
    else:
        raise ValueError(f"Unsupported mode: {mode}")

    return HFDataset.from_dict(data_dict)


# ====================
# Training Functions
# ====================

def train_sft(config: TrainingConfig):
    """
    Supervised Fine-Tuning Phase

    This trains the model on ALL instruction hierarchy data to:
    1. Follow aligned instructions at all hierarchy levels
    2. Ignore/refuse misaligned instructions
    3. Extract and follow compositional instructions
    """
    print("=" * 60)
    print("Starting Supervised Fine-Tuning (SFT)")
    print("=" * 60)

    # Load tokenizer
    print(f"\n1. Loading tokenizer from {config.model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # Add special tokens for hierarchy
    special_tokens = {
        'additional_special_tokens': ['<|system|>', '<|user|>', '<|tool|>', '<|assistant|>']
    }
    num_added = tokenizer.add_special_tokens(special_tokens)
    print(f"Added {num_added} special tokens for instruction hierarchy")

    # Quantization config
    if config.use_4bit:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
    else:
        bnb_config = None

    # Load model
    print(f"\n2. Loading base model {config.model_name}...")
    model = AutoModelForCausalLM.from_pretrained(
        config.model_name,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.bfloat16,
    )

    # Resize embeddings to accommodate new tokens
    model.resize_token_embeddings(len(tokenizer))
    print(f"Resized model embeddings to {len(tokenizer)}")

    # Prepare model for LoRA
    if config.use_4bit:
        model = prepare_model_for_kbit_training(model)

    # LoRA config
    print("\n3. Configuring LoRA...")
    lora_config = LoraConfig(
        r=config.lora_r,
        lora_alpha=config.lora_alpha,
        target_modules=config.lora_target_modules,
        lora_dropout=config.lora_dropout,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # Load dataset - use the full combined dataset
    print(f"\n4. Loading training data from {config.full_dataset_file}...")
    train_dataset = InstructionHierarchyDataset(
        data_path=config.full_dataset_file,
        tokenizer=tokenizer,
        max_length=config.max_length,
        mode="sft"
    )

    # Training arguments
    training_args = TrainingArguments(
        output_dir=f"{config.output_dir}/sft",
        per_device_train_batch_size=config.batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        warmup_steps=config.warmup_steps,
        num_train_epochs=config.num_epochs_sft,
        learning_rate=config.learning_rate,
        bf16=True,
        logging_steps=10,
        save_strategy="epoch",
        save_total_limit=2,
        report_to="wandb" if config.use_wandb else "none",
    )

    # Create trainer
    print("\n5. Initializing Trainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
    )

    # Train
    print("\n6. Starting training...")
    trainer.train()

    # Save final model
    final_checkpoint_path = f"{config.output_dir}/sft_final"
    print(f"\n7. Saving model to {final_checkpoint_path}")
    trainer.save_model(final_checkpoint_path)
    tokenizer.save_pretrained(final_checkpoint_path)

    print("=" * 60)
    print("SFT training completed!")
    print("=" * 60)


def train_dpo(config: TrainingConfig):
    """
    Direct Preference Optimization Phase

    This further refines the SFT model using preference learning to:
    1. Strongly prefer refusals over compliance for attacks
    2. Prefer helpful responses over over-refusals for aligned instructions
    """
    print("=" * 60)
    print("Starting Direct Preference Optimization (DPO)")
    print("=" * 60)

    # Path to SFT checkpoint
    sft_model_path = f"{config.output_dir}/sft_final"

    if not os.path.exists(sft_model_path):
        raise ValueError(
            f"SFT checkpoint not found at {sft_model_path}. "
            f"Please run SFT training first with --mode sft or --mode both"
        )

    print(f"\n1. Loading SFT checkpoint from: {sft_model_path}")

    # Load tokenizer from SFT checkpoint
    tokenizer = AutoTokenizer.from_pretrained(sft_model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # Quantization config
    if config.use_4bit:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
    else:
        bnb_config = None

    # Load base model for training
    print("\n2. Loading base model for training...")
    model = AutoModelForCausalLM.from_pretrained(
        config.model_name,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.bfloat16,
    )
    model.resize_token_embeddings(len(tokenizer))

    # Load SFT adapter
    print("Loading SFT adapter onto training model...")
    model = PeftModel.from_pretrained(model, sft_model_path)

    # Load reference model (frozen copy of SFT model)
    print("\n3. Loading base model for reference...")
    ref_model = AutoModelForCausalLM.from_pretrained(
        config.model_name,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.bfloat16,
    )
    ref_model.resize_token_embeddings(len(tokenizer))

    print("Loading SFT adapter onto reference model...")
    ref_model = PeftModel.from_pretrained(ref_model, sft_model_path)

    # Load DPO dataset
    print(f"\n4. Loading DPO training data from {config.full_dataset_file}...")
    dpo_dataset_obj = InstructionHierarchyDataset(
        data_path=config.full_dataset_file,
        tokenizer=tokenizer,
        max_length=config.max_length,
        mode="dpo"
    )

    # Convert to HuggingFace Dataset
    train_dataset = create_hf_dataset(dpo_dataset_obj.examples, mode="dpo")
    print(f"Created {len(train_dataset)} DPO training examples")

    # DPO Training configuration
    print("\n5. Configuring DPO training...")
    dpo_config = DPOConfig(
        output_dir=f"{config.output_dir}/dpo",
        per_device_train_batch_size=config.batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        warmup_steps=config.warmup_steps,
        num_train_epochs=config.num_epochs_dpo,
        learning_rate=config.learning_rate,
        bf16=True,
        logging_steps=10,
        save_strategy="epoch",
        save_total_limit=2,
        report_to="wandb" if config.use_wandb else "none",
        beta=config.dpo_beta,
        max_length=config.max_length,
        max_prompt_length=config.max_prompt_length,
    )

    # Create DPO Trainer
    print("\n6. Initializing DPO Trainer...")
    dpo_trainer = DPOTrainer(
        model=model,
        ref_model=ref_model,
        args=dpo_config,
        train_dataset=train_dataset
    )

    # Train
    print("\n7. Starting DPO training...")
    dpo_trainer.train()

    # Save final model
    final_checkpoint_path = f"{config.output_dir}/dpo_final"
    print(f"\n8. Saving DPO model to {final_checkpoint_path}")
    dpo_trainer.save_model(final_checkpoint_path)
    tokenizer.save_pretrained(final_checkpoint_path)

    print("=" * 60)
    print("DPO training completed!")
    print("=" * 60)


# ====================
# Main Function
# ====================

def main():
    """Main training function with command line arguments"""
    parser = argparse.ArgumentParser(
        description="Train Instruction Hierarchy Model - Single unified model on all attack types"
    )
    parser.add_argument(
        "--mode",
        choices=["sft", "dpo", "both"],
        required=True,
        help="Training mode: 'sft' for supervised fine-tuning, 'dpo' for DPO, 'both' for sequential training"
    )
    parser.add_argument(
        "--data_dir",
        default="./instruction_hierarchy_data",
        help="Directory containing generated training data"
    )
    parser.add_argument(
        "--output_dir",
        default="./instruction_hierarchy_model",
        help="Output directory for model checkpoints"
    )
    parser.add_argument(
        "--model_name",
        default="Qwen/Qwen2.5-7B-Instruct",
        help="Base model name"
    )
    parser.add_argument(
        "--use_wandb",
        action="store_true",
        help="Enable Weights & Biases logging"
    )
    parser.add_argument(
        "--wandb_project",
        default="instruction-hierarchy",
        help="W&B project name"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1,
        help="Batch size per device"
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=2e-5,
        help="Learning rate"
    )
    parser.add_argument(
        "--num_epochs_sft",
        type=int,
        default=3,
        help="Number of epochs for SFT training"
    )
    parser.add_argument(
        "--num_epochs_dpo",
        type=int,
        default=1,
        help="Number of epochs for DPO training"
    )

    args = parser.parse_args()

    # Initialize configuration
    config = TrainingConfig()
    config.data_dir = args.data_dir
    config.full_dataset_file = os.path.join(args.data_dir, "instruction_hierarchy_full.json")
    config.output_dir = args.output_dir
    config.model_name = args.model_name
    config.use_wandb = args.use_wandb
    config.wandb_project = args.wandb_project
    config.batch_size = args.batch_size
    config.learning_rate = args.learning_rate
    config.num_epochs_sft = args.num_epochs_sft
    config.num_epochs_dpo = args.num_epochs_dpo

    # Verify data exists
    if not os.path.exists(config.full_dataset_file):
        raise FileNotFoundError(
            f"Training data not found at {config.full_dataset_file}. "
            f"Please run generate_data.py first to create the training data."
        )

    # Create output directory
    os.makedirs(config.output_dir, exist_ok=True)

    # Initialize wandb if enabled
    if config.use_wandb:
        wandb.init(project=config.wandb_project, name=f"instruction-hierarchy-{args.mode}")

    print("\n" + "=" * 60)
    print("INSTRUCTION HIERARCHY TRAINING")
    print("=" * 60)
    print(f"Mode: {args.mode}")
    print(f"Data: {config.full_dataset_file}")
    print(f"Output: {config.output_dir}")
    print(f"Model: {config.model_name}")
    print("=" * 60 + "\n")

    try:
        if args.mode == "sft":
            print("Running SFT training only...")
            train_sft(config)

        elif args.mode == "dpo":
            print("Running DPO training only...")
            train_dpo(config)

        elif args.mode == "both":
            print("Running both SFT and DPO training...")
            train_sft(config)
            print("\n\n" + "=" * 60)
            print("SFT Complete - Starting DPO")
            print("=" * 60 + "\n")
            train_dpo(config)

        print("\n" + "=" * 60)
        print("✓ Training completed successfully!")
        print(f"✓ Final model saved to: {config.output_dir}")
        print("=" * 60)

    except Exception as e:
        print(f"\n✗ Training failed with error: {e}")
        import traceback
        traceback.print_exc()
        raise

    finally:
        if config.use_wandb:
            wandb.finish()


if __name__ == "__main__":
    main()